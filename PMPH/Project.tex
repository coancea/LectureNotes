\documentclass{beamer}

%% \documentclass[handout]{beamer}
%% % use this with the [handout] option to create handouts for the audience
%% \usepackage{pgfpages}
%% \pgfpagesuselayout{2 on 1}[a4paper,border shrink=5mm]

\mode<presentation>
{
  \usetheme{Diku}
% set this to your preferences:
  \setbeamercovered{invisible}
%  \setbeamercovered{transparent}
}

\usepackage{graphicx}
\usepackage{epic}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\basetop}[1]{\vtop{\vskip-1ex\hbox{#1}}}
\newcommand{\source}[1]{\let\thefootnote\relax\footnotetext{\scriptsize\textcolor{kugray1}{Source: #1}}}

% for coloured code citation in text:
\usepackage{fancyvrb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%    code sections   %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% code highlighting commands in own block
\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\scriptsize}
\DefineVerbatimEnvironment{icode}{Verbatim}{fontsize=\scriptsize}

% Fancy code with color commands:
\DefineVerbatimEnvironment{colorcode}%
        {Verbatim}{fontsize=\scriptsize,commandchars=\\\{\}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%    some coloring    %%%%%%%%

\definecolor{Red}{RGB}{220,50,10}
\definecolor{Blue}{RGB}{0,51,102}
\definecolor{Yellow}{RGB}{102,51,0}
\definecolor{Orange}{RGB}{178,36,36}
\definecolor{Grey}{RGB}{180,180,180}
\definecolor{Green}{RGB}{20,120,20}
\definecolor{Purple}{RGB}{160,50,100}
\newcommand{\red}[1]{\textcolor{Red}{{#1}}}
\newcommand{\blue}[1]{\textcolor{Blue}{{#1}}}
\newcommand{\yellow}[1]{\textcolor{Yellow}{{#1}}}
\newcommand{\orange}[1]{\textcolor{Orange}{{#1}}}
\newcommand{\grey}[1]{\textcolor{Grey}{{#1}}}
\newcommand{\green}[1]{\textcolor{Green}{{#1}}}
\newcommand{\purple}[1]{\textcolor{Purple}{{#1}}}




% use "DIKU green" from our color theme for \emph
\renewcommand{\emph}[1]{\textcolor{structure}{#1}}
% use some not-too-bright red for an \emp command
\definecolor{DikuRed}{RGB}{130,50,32}
\newcommand{\emp}[1]{\textcolor{DikuRed}{ #1}}
\definecolor{CosGreen}{RGB}{10,100,70}
\newcommand{\emphh}[1]{\textcolor{CosGreen}{ #1}}
\definecolor{CosBlue}{RGB}{55,111,122}
\newcommand{\emphb}[1]{\textcolor{CosBlue}{ #1}}
\definecolor{CosRed}{RGB}{253,1,1}
\newcommand{\empr}[1]{\textcolor{CosRed}{ #1}}

\newcommand{\mymath}[1]{$ #1 $}
\newcommand{\myindx}[1]{_{#1}}
\newcommand{\myindu}[1]{^{#1}}

\newcommand{\Fasto}{\textsc{Fasto}\xspace}


%%%%%%%%%%%%%%%%%%%%

\title[Project]{Project Related Discussion}

\author[C.~Oancea]{Cosmin E. Oancea\\{\tt cosmin.oancea@diku.dk}}

\institute{Department of Computer Science (DIKU)\\University of Copenhagen}


\date[Sept 2014]{September 2014 PMPH Lecture Notes}


\begin{document}

\titleslide


\begin{frame}[fragile]
	\tableofcontents
\end{frame}

%%%%%%%% real content starts here %%%%%%%%%%

\section{Code Structure}

\begin{frame}[fragile,t]
  \frametitle{Datasets} % of CPU, Multicores, GPGPU
\begin{itemize}
    \item[Small:] OUTER=16, NUM\_X=32, NUM\_Y=256, NUM\_T=90
    \item{Medium:} OUTER=32, NUM\_X=47, NUM\_Y=181, NUM\_T=93
    \item{Large:} OUTER=128, NUM\_X=256, NUM\_Y=256, NUM\_T=128
\end{itemize}

\end{frame}


\begin{frame}[fragile,t]
  \frametitle{Code Structure} % of CPU, Multicores, GPGPU

\begin{block}{Code Entry Point}
\begin{columns}
\column{0.48\textwidth}
\begin{colorcode}
void   run_OrigCPU(...) \{
  REAL strike;
  PrivGlobs globs(numX,numY,numT);
  \emph{for(int i=0; i<outer; ++ i)} \{
    strike = 0.001*i;
    res[i] = \emp{value}( globs,s0,strike,t,
                    alpha,nu,   beta,
                    numX, numY, numT );
  \}
\}
\end{colorcode}
\column{0.48\textwidth}
\begin{colorcode}
REAL   \emp{value}( ... ) \{
  initGrid(s0,alpha,nu,t, 
           numX,numY,numT, 
           globs);
  initOperator(globs.myX,
               globs.myDxx);
  initOperator(globs.myY,
               globs.myDyy);
  setPayoff(strike, globs);
  \alert{for(int i=numT-2;i>=0;--i)}\{
    updateParams(i,alpha,beta,
                 nu,globs);
    rollback(i, globs);
  \}
  return globs.myResult[globs.myXindex]
                       [globs.myYindex];
\}
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}


\begin{frame}[fragile,t]
  \frametitle{Loop Nests} % of CPU, Multicores, GPGPU

\begin{block}{Loop Nests}
\begin{columns}
\column{0.95\textwidth}
\begin{colorcode}
rollback( ... ) \{
  vector<vector<REAL> > u(numY, vector<REAL>(numX));   // [numY][numX]
  vector<vector<REAL> > v(numX, vector<REAL>(numY));   // [numX][numY]
  vector<REAL> a(numZ), b(numZ), c(numZ), y(numZ);     // [max(numX,numY)] 
  vector<REAL> yy(numZ);  // temporary used in tridag  // [max(numX,numY)]
  for(i=0;i<numX;i++) \{
    for(j=0;j<numY;j++) \{
      u[j][i] = dtInv*\blue{globs.\alert{myResult[i][j]}};
  \} \}  .......
  // implicit y
  for(i=0;i<numX;i++) \{ 
    for(j=0;j<numY;j++) \{  // here a, b, c should have size [numY]
      a[j] =		 - 0.5*(0.5*globs.myVarY[i][j]*globs.myDyy[j][0]);
      b[j] = dtInv - 0.5*(0.5*globs.myVarY[i][j]*globs.myDyy[j][1]);
      c[j] =		 - 0.5*(0.5*globs.myVarY[i][j]*globs.myDyy[j][2]);
    \}
    for(j=0;j<numY;j++)
      y[j] = dtInv*u[j][i] - 0.5*v[i][j];
    // here yy should have size [numY]
    tridag(a,b,c,y,numY,globs.\alert{myResult[i]},yy);
  \} \}
\end{colorcode}
\column{0.05\textwidth}
\end{columns}
\end{block} 

\end{frame}


\begin{frame}[fragile,t]
  \frametitle{How To Parallelize} % of CPU, Multicores, GPGPU

\begin{itemize}
    \item summarize accesses inter-procedurally. 
            For each loop what does it write and what does it read?\medskip
    \item Within each loop: are all reads covered by writes
            executed within the same iteration?
            If so then privatization solves those dependencies!\medskip
    \item For CUDA: do array expansion instead of privatization.\medskip
    \item Decide for each loop whether it can or cannot be parallelize.\medskip
    \item Use loop distribution to create perfect nests,
            which will become later your CUDA kernels.\medskip
    \item Use loop interchange and/or matrix transposition
            to obtain coalesced access to global memory.
\end{itemize}
\end{frame}

\section{CPU parallelization}

\begin{frame}[fragile]
	\tableofcontents[currentsection]
\end{frame}

\begin{frame}[fragile,t]
  \frametitle{CPU Parallelization} % of CPU, Multicores, GPGPU

In function {\tt run\_OrigCPU} move the declaration of
{\tt strike} and {\tt globs} inside the loop, and parallelize
the loop via an \textsc{OpenMP} pragma:

\begin{block}{Parallelizing the Outermost Loop Via OpenMP}
\begin{columns}
\column{0.95\textwidth}
\begin{colorcode}
\emphh{\#pragma omp parallel for default(shared) schedule(static)}
    for( unsigned i = 0; i < outer; ++ i ) \{
        REAL strike;
        PrivGlobs    globs(numX, numY, numT);

        strike = 0.001*i;
        res[i] = value( globs, s0, strike, t,
                        alpha, nu,    beta,
                        numX,  numY,  numT );
    \}
\end{colorcode}
\column{0.05\textwidth}
\end{columns}
\end{block} 

\alert{Explain why this is safe in the report!}\\
(For example if you not move the declarations inside
the loop and still parallelize the loop, the executions
will NOT validate).

\end{frame}


\section{Code Transformations}

\begin{frame}[fragile]
	\tableofcontents[currentsection]
\end{frame}

\subsection{Reasoning About Parallelism: Privatization \& Array Expansion}

\begin{frame}[fragile,t]
  \frametitle{Reasoning About Parallelism: Privatization} % of CPU, Multicores, GPGPU


\begin{itemize}
\item The outermost loop of index {\tt i} is NOT parallel as it is,
because all its iterations write and read all indices of
array {\tt A}.\smallskip

\item However, each iteration reads (is covered by) what was 
written in the same iteration, a.k.a., array {\tt A} can be privatized.

\item Privatization can be achieved by moving the
declaration of {\tt a} inside the outermost loop
(each iteration works with its own private version of {\tt A}).
\end{itemize}

\begin{block}{Parallelizing the Outermost Loop Via OpenMP}
\begin{columns}
\column{0.47\textwidth}
\begin{colorcode}
float A[N];
\emp{for(int i=0;i<N;i++)\{ //seq}
  for(int j=0;j<N;j++)\{
    A[j] = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = A[j]
  \}  
\}
\end{colorcode}
\column{0.47\textwidth}
\begin{colorcode}
\emphh{for(int i=0;i<N;i++)\{ //par}
  float A[N];
  for(int j=0;j<N;j++)\{
    A[j] = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = A[j]
  \}  
\}
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}

\begin{frame}[fragile,t]
  \frametitle{Array Expansion} % of CPU, Multicores, GPGPU


\begin{itemize}
\item In CUDA it is preferable that all memory is allocated
        before the kernel starts, hence making array {\tt A}
        local would not work.\smallskip

\item Instead, expand array {\tt A} with an extra dimension,
        whose size is the count of the outermost loop. 

\item Now iteration {\tt i} has exclusive access, i.e., writes to and 
        reads from,  row {\tt i} of expanded array {\tt A}.

\item The two versions of code below are \emp{semantically equivalent!}
\end{itemize}

\begin{block}{Semantically Equivalent: Privatized {\em vs} Expanded {\tt A}}
\begin{columns}
\column{0.47\textwidth}
\begin{colorcode}
\emphh{for(int i=0;i<N;i++)\{ //par}
  \emp{float A[N];}
  for(int j=0;j<N;j++)\{
    \emp{A[j]} = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = \emp{A[j]}
  \}  
\}
\end{colorcode}
\column{0.47\textwidth}
\begin{colorcode}
\emphh{float A[N, N];}
\emphh{for(int i=0;i<N;i++)\{ //par}
  for(int j=0;j<N;j++)\{
    \emphh{A[i,j]} = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = \emphh{A[i,j]}
  \}  
\}
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}


\subsection{Creating CUDA Kernels via Loop Distribution}

\end{document}

