\documentclass{beamer}

%% \documentclass[handout]{beamer}
%% % use this with the [handout] option to create handouts for the audience
%% \usepackage{pgfpages}
%% \pgfpagesuselayout{2 on 1}[a4paper,border shrink=5mm]

\mode<presentation>
{
  \usetheme{Diku}
% set this to your preferences:
  \setbeamercovered{invisible}
%  \setbeamercovered{transparent}
}

\usepackage{graphicx}
\usepackage{epic}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\basetop}[1]{\vtop{\vskip-1ex\hbox{#1}}}
\newcommand{\source}[1]{\let\thefootnote\relax\footnotetext{\scriptsize\textcolor{kugray1}{Source: #1}}}

% for coloured code citation in text:
\usepackage{fancyvrb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%    code sections   %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% code highlighting commands in own block
\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\scriptsize}
\DefineVerbatimEnvironment{icode}{Verbatim}{fontsize=\scriptsize}

% Fancy code with color commands:
\DefineVerbatimEnvironment{colorcode}%
        {Verbatim}{fontsize=\scriptsize,commandchars=\\\{\}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%    some coloring    %%%%%%%%

\definecolor{Red}{RGB}{220,50,10}
\definecolor{Blue}{RGB}{0,51,102}
\definecolor{Yellow}{RGB}{102,51,0}
\definecolor{Orange}{RGB}{178,36,36}
\definecolor{Grey}{RGB}{180,180,180}
\definecolor{Green}{RGB}{20,120,20}
\definecolor{Purple}{RGB}{160,50,100}
\newcommand{\red}[1]{\textcolor{Red}{{#1}}}
\newcommand{\blue}[1]{\textcolor{Blue}{{#1}}}
\newcommand{\yellow}[1]{\textcolor{Yellow}{{#1}}}
\newcommand{\orange}[1]{\textcolor{Orange}{{#1}}}
\newcommand{\grey}[1]{\textcolor{Grey}{{#1}}}
\newcommand{\green}[1]{\textcolor{Green}{{#1}}}
\newcommand{\purple}[1]{\textcolor{Purple}{{#1}}}




% use "DIKU green" from our color theme for \emph
\renewcommand{\emph}[1]{\textcolor{structure}{#1}}
% use some not-too-bright red for an \emp command
\definecolor{DikuRed}{RGB}{130,50,32}
\newcommand{\emp}[1]{\textcolor{DikuRed}{ #1}}
\definecolor{CosGreen}{RGB}{10,100,70}
\newcommand{\emphh}[1]{\textcolor{CosGreen}{ #1}}
\definecolor{CosBlue}{RGB}{55,111,122}
\newcommand{\emphb}[1]{\textcolor{CosBlue}{ #1}}
\definecolor{CosRed}{RGB}{253,1,1}
\newcommand{\empr}[1]{\textcolor{CosRed}{ #1}}

\newcommand{\mymath}[1]{$ #1 $}
\newcommand{\myindx}[1]{_{#1}}
\newcommand{\myindu}[1]{^{#1}}

\newcommand{\Fasto}{\textsc{Fasto}\xspace}


%%%%%%%%%%%%%%%%%%%%

\title[LocVolCalib]{LovVolCalib Project Discussion}

\author[C.~Oancea]{Cosmin E. Oancea\\{\tt cosmin.oancea@diku.dk}}

\institute{Department of Computer Science (DIKU)\\University of Copenhagen}


\date[Oct 2018]{October 2018 PMPH Project Slides}


\begin{document}

\titleslide


\begin{frame}[fragile]
	\tableofcontents
\end{frame}

%%%%%%%% real content starts here %%%%%%%%%%

\section{Code Structure}

\begin{frame}[fragile,t]
  \frametitle{Datasets} % of CPU, Multicores, GPGPU
\begin{itemize}
    \item{Small:} OUTER=16, NUM\_X=32, NUM\_Y=256, NUM\_T=90\medskip
    \item{Medium:} OUTER=32, NUM\_X=47, NUM\_Y=181, NUM\_T=93\medskip
    \item{Large:} OUTER=128, NUM\_X=256, NUM\_Y=256, NUM\_T=128
\end{itemize}\bigskip

\medskip 
Target primarily the {\em large} dataset, in which the map-like parallel 
dimensions ({\tt OUTER} and ({\tt NUM\_X} or {\tt NUM\_Y})) offer enough 
parallelism to fully utilize the hardware.

\medskip

As time permits, have a go to parallelizing all three parallel dimensions, 
i.e., {\sc tridag} can be re-written based on segmented {\tt scan}s.  
The target for this is the {\em small} dataset, which, otherwise, does not 
has enough parallelism to fully utilize the hardware.

\medskip

\alert{Make sure to optimize global-memory accesses such that they are either {\em coalesced},
or efficiently cached (if applicable). This is paramount for achieving good performance!}

\end{frame}


\begin{frame}[fragile,t]
  \frametitle{Code Structure} % of CPU, Multicores, GPGPU

\begin{block}{Code Entry Point}
\begin{columns}
\column{0.47\textwidth}
\begin{colorcode}
void   run_OrigCPU(...) \{
  REAL strike;
  PrivGlobs globs(numX,numY,numT);
  \emph{for(int i=0; i<outer; ++ i)} \{
    strike = 0.001*i;
    res[i] = \emp{value}( globs,s0,strike,t,
                    alpha,nu,   beta,
                    numX, numY, numT );
  \}
\}
\end{colorcode}
\column{0.5\textwidth}
\begin{colorcode}
REAL   \emp{value}( ... ) \{
  initGrid(s0,alpha,nu,t, 
           numX,numY,numT, 
           globs);
  initOperator(globs.myX,
               globs.myDxx);
  initOperator(globs.myY,
               globs.myDyy);
  setPayoff(strike, globs);
  \alert{for(int i=numT-2;i>=0;--i)}\{
    updateParams(i,alpha,beta,
                 nu,globs);
    rollback(i, globs);
  \}
  return globs.myResult[globs.myXindex]
                       [globs.myYindex];
\}
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}


\begin{frame}[fragile,t]
  \frametitle{Loop Nests} % of CPU, Multicores, GPGPU

\begin{block}{Loop Nests}
\begin{columns}
\column{0.95\textwidth}
\begin{colorcode}
rollback( ... ) \{
  vector<vector<REAL> > u(numY, vector<REAL>(numX));   // [numY][numX]
  vector<vector<REAL> > v(numX, vector<REAL>(numY));   // [numX][numY]
  vector<REAL> a(numZ), b(numZ), c(numZ), y(numZ);     // [max(numX,numY)] 
  vector<REAL> yy(numZ);  // temporary used in tridag  // [max(numX,numY)]
  for(i=0;i<numX;i++) \{
    for(j=0;j<numY;j++) \{
      u[j][i] = dtInv*\blue{globs.\alert{myResult[i][j]}};
  \} \}  .......
  // implicit y
  for(i=0;i<numX;i++) \{ 
    for(j=0;j<numY;j++) \{  // here a, b, c should have size [numY]
      a[j] =		 - 0.5*(0.5*globs.myVarY[i][j]*globs.myDyy[j][0]);
      b[j] = dtInv - 0.5*(0.5*globs.myVarY[i][j]*globs.myDyy[j][1]);
      c[j] =		 - 0.5*(0.5*globs.myVarY[i][j]*globs.myDyy[j][2]);
    \}
    for(j=0;j<numY;j++)
      y[j] = dtInv*u[j][i] - 0.5*v[i][j];
    // here yy should have size [numY]
    tridag(a,b,c,y,numY,globs.\alert{myResult[i]},yy);
  \} \}
\end{colorcode}
\column{0.05\textwidth}
\end{columns}
\end{block} 

\end{frame}


\begin{frame}[fragile,t]
  \frametitle{How To Parallelize} % of CPU, Multicores, GPGPU

\begin{itemize}
    \item summarize accesses inter-procedurally. 
            For each loop what does it write and what does it read?\medskip
    \item Within each loop: are all reads covered by writes
            executed within the same iteration?
            If so then privatization solves those dependencies!\medskip
    \item For CUDA: do array expansion instead of privatization.\medskip
    \item Decide for each loop whether it can or cannot be parallelize.\medskip
    \item Use loop distribution to create perfect nests,
            which will become later your CUDA kernels.\medskip
    \item Use loop interchange and/or matrix transposition
            to obtain coalesced access to global memory.
\end{itemize}
\end{frame}

\section{CPU parallelization}

\begin{frame}[fragile]
	\tableofcontents[currentsection]
\end{frame}

\begin{frame}[fragile,t]
  \frametitle{CPU Parallelization} % of CPU, Multicores, GPGPU

In function {\tt run\_OrigCPU} move the declaration of
{\tt strike} and {\tt globs} inside the loop, and parallelize
the loop via an \textsc{OpenMP} pragma:

\begin{block}{Parallelizing the Outermost Loop Via OpenMP}
\begin{columns}
\column{0.95\textwidth}
\begin{colorcode}
\emphh{\#pragma omp parallel for default(shared) schedule(static)}
    for( unsigned i = 0; i < outer; ++ i ) \{
        REAL strike;
        PrivGlobs    globs(numX, numY, numT);

        strike = 0.001*i;
        res[i] = value( globs, s0, strike, t,
                        alpha, nu,    beta,
                        numX,  numY,  numT );
    \}
\end{colorcode}
\column{0.05\textwidth}
\end{columns}
\end{block} 

\alert{Explain why this is safe in the report!}\\
(For example if you do NOT move the declarations inside
the loop and still parallelize the loop, the execution
will NOT validate).

\end{frame}


\section{Code Transformations}

\begin{frame}[fragile]
	\tableofcontents[currentsection]
\end{frame}

\subsection{Reasoning About Parallelism: Privatization \& Array Expansion}

\begin{frame}[fragile,t]
  \frametitle{Reasoning About Parallelism: Privatization} % of CPU, Multicores, GPGPU

\vspace{-1ex}
\begin{block}{Parallelizing the Outermost Loop Via Privatization}
\begin{columns}
\column{0.47\textwidth}
\begin{colorcode}
float A[N];
\emp{for(int i=0;i<M;i++)\{ //seq}
  for(int j=0;j<N;j++)\{
    A[j] = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = A[j]
  \}  
\}
\end{colorcode}
\column{0.47\textwidth}
\begin{colorcode}
\emphh{for(int i=0;i<M;i++)\{ //par}
  float A[N];
  for(int j=0;j<N;j++)\{
    A[j] = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = A[j]
  \}  
\}
\end{colorcode}
\end{columns}
\end{block} 

\begin{itemize}
\item The outermost loop of index {\tt i} is NOT parallel as it is,
because all its iterations write and read all indices of
array {\tt A}.\smallskip

\item However, the  iteration reads (is covered by) what was 
written in the same iteration, a.k.a., array {\tt A} can be privatized.

\item Privatization can be achieved by moving the
declaration of {\tt a} inside the outermost loop
(each iteration works with its own private version of {\tt A}).
\end{itemize}

\end{frame}

\begin{frame}[fragile,t]
  \frametitle{Array Expansion} % of CPU, Multicores, GPGPU

\vspace{-1ex}
\begin{block}{Semantically Equivalent: Privatized {\em vs} Expanded {\tt A}}
\begin{columns}
\column{0.47\textwidth}
\begin{colorcode}
\emphh{for(int i=0;i<M;i++)\{ //par}
  \emp{float A[N];}
  for(int j=0;j<N;j++)\{
    \emp{A[j]} = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = \emp{A[j]}
  \}  
\}
\end{colorcode}
\column{0.47\textwidth}
\begin{colorcode}
\emphh{float A[M, N];}
\emphh{for(int i=0;i<M;i++)\{ //par}
  for(int j=0;j<N;j++)\{
    \emphh{A[i,j]} = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = \emphh{A[i,j]}
  \}  
\}
\end{colorcode}
\end{columns}
\end{block} 

\begin{itemize}
\item In CUDA it is preferable that all memory is allocated
        before the kernel starts, hence making array {\tt A}
        local would not work.\smallskip

\item Instead, expand array {\tt A} with an extra (outermost) dimension,
        whose size is the count of the outermost loop. 

\item Now iteration {\tt i} has exclusive access, i.e., writes to and 
        reads from,  row {\tt i} of expanded array {\tt A}.

\item The two versions of code below are \emp{semantically equivalent!}
\end{itemize}


\end{frame}



\subsection{Creating CUDA Kernels via Loop Distribution}

\begin{frame}[fragile]
	\tableofcontents[currentsubsection]
\end{frame}


\begin{frame}[fragile,t]
  \frametitle{Create CUDA Kernels via Loop Distribution} % of CPU, Multicores, GPGPU


\begin{itemize}
\item \emp{Theorem:} A parallel loop can be distributed across
                its statements (guaranteed that its
                dependency graph does not have cycles).\smallskip

\item CUDA kernels are obtained by distributing the outer loop 
                around the inner loops (in order to improve
                the degree of parallelism). 
\end{itemize}

\begin{block}{Degree of parallelism M{\tt~~~}{\em vs.}{\tt~~~} Degree of parallelism: M*N}
\begin{columns}
\column{0.47\textwidth}
\begin{colorcode}
\emphh{float A[M, N];}
\emphh{for(int i=0;i<M;i++)\{ //par}
  for(int j=0;j<N;j++)\{
    A[i,j] = ...
  \}
  ...
  for(int j=0;j<N;j++)\{
    ... = A[i,j]
  \}  
\}
\end{colorcode}
\column{0.47\textwidth}
\begin{colorcode}
\emphh{float A[M, N];}
\emphh{for(int i=0;i<M;i++)\{ //par}
  \emphh{for(int j=0;j<N;j++)\{ //par}
    A[i,j] = ...; \emp{// 2D CUDA kernel}
\} \}
\emphh{for(int i=0;i<M;i++)\{ //par}
  ...
\}
\emphh{for(int i=0;i<M;i++)\{ //par}
  \emphh{for(int j=0;j<N;j++)\{ //par}
    ... = A[i,j]; \emp{// 2D CUDA Kernel}
\} \}
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}


\begin{frame}[fragile,t]
  \frametitle{Inline Simple Expression {\em vs} Array Expansion} % of CPU, Multicores, GPGPU


\begin{itemize}
\item Loop distribution requires array expansion of the local variables.\smallskip

\item If the local variable is a simple scalar expression it is
            better to inline that expression rather than creating an array
            for it.\smallskip
\item Use your better judgment when to distribute and when to inline, 
            i.e., do not create too many arrays
            (tradeoff between redundant computation AND extra memory \& global accesses)
\end{itemize}

\begin{block}{Inline Scalar Variables {\tt~~~}Rather Than{\tt~~~} Array Expansion}
\begin{columns}
\column{0.42\textwidth}\vspace{-2ex}
\begin{colorcode}
\emphh{float A[M, N];}
\emphh{for(int i=0;i<M;i++)\{ //par}
  \emp{int tmp = i*i;}
  for(int j=0;j<N;j++)\{
    A[i,j] = ... * tmp;
  \}
\}  \mymath{\downarrow \ {\tt inline scalar exp} \ \downarrow}
float A[M, N];
\emphh{for(int i=0;i<M;i++) //par}
  \emphh{for(int j=0;j<N;j++)\{//par}
    A[i,j] = ... * \blue{((float)i*i)};
  \}
\end{colorcode}
\column{0.55\textwidth}\vspace{-2ex}
\begin{colorcode}
// Systematic distribution will create
// Many Arrays, and Many access to Global 
// Memory. (It might be cheaper to do some 
// \mymath{\leftarrow} redundant computation instead).
\emp{float tmps[M];}
float A[M, N];
\emphh{for(int i=0;i<M;i++) //par}
  \emp{tmps[i]} = (float)(i*i);
\emphh{for(int i=0;i<M;i++) //par}
  \emphh{for(int j=0;j<N;j++)\{//par}
    A[i,j] = ... * \emp{tmps[i]};
  \}
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}


\subsection{Various Optimizations, e.g., Coalesced Memory}

\begin{frame}[fragile]
	\tableofcontents[currentsubsection]
\end{frame}


\begin{frame}[fragile,t]
  \frametitle{Optimising CUDA Kernel (Memory Coalescing)} % of CPU, Multicores, GPGPU


\begin{itemize}
\item After creating the CUDA Kernels, one might also want to
        optimise them, for example
\item Coalesced Access to global memory may be obtained via
        loop interchange or (segmented) matrix transposition. 
\end{itemize}
\vspace{-2ex}
\begin{block}{Coalesced Access via Loop Interchange or Matrix Transposition}
\begin{columns}
\column{0.47\textwidth}
\begin{colorcode}
\emp{float A[M, N];}
\emphh{for(int j=0;j<N;j++)\{ //par}
  \emphh{for(int i=0;i<M;i++)\{ //par}
    \emp{A[i,j] = ... // uncoalesced access} 
\} \}  \mymath{\downarrow \ {\tt loop interchange} \ \downarrow}

\emphh{for(int i=0;i<M;i++)\{ //par}
  \emphh{for(int j=0;j<N;j++)\{ //par}
    \blue{A[i,j] = ... // coalesced access} 
\} \}
\end{colorcode}
\column{0.47\textwidth}
\begin{colorcode}
// Fixing uncoalesced accesses via 
// matrix transposition
\blue{float Atr[N, M];}
\emphh{for(int j=0;j<N;j++)\{ //par}
  \emphh{for(int i=0;i<M;i++)\{ //par}
    \blue{Atr[j,i] = ... // coalesced} 
\} \}
float A[M,N];
\blue{A=transpose(Atr); //Atr[j,i]\mymath{\equiv}A[i,j]}
\end{colorcode}
\end{columns}
\end{block} 

\emp{Note that applying loop interchange may make some uncoalesced
accesses coalesced, but it might also make other (originally) coalesced
accesses uncoalesced. In those cases use TRANSPOSITION.}


\end{frame}


\begin{frame}[fragile,t]
  \frametitle{You May Need Segmented Transpose} % of CPU, Multicores, GPGPU


\begin{itemize}
\item Project uses three dimensional arrays,
        i.e., an array of matrices, and requires transposing
        each matrix (the two inermost dims).\smallskip

\item Nothing to be afraid of -- this corresponds to 
        a three dimensional CUDA kernel in which you 
        write the matrix-transposition code for the innermost
        two dimensions. Pseudocode below:  
\end{itemize}

\begin{block}{Segmented Transposition: Sequential and CUDA Kernel}
\begin{columns}
\column{0.37\textwidth}\vspace{-2ex}
\begin{colorcode}
\emp{float A[O, M, N];}
\emphh{for(int k=0;k<O;k++)\{ //par}
  \emphh{for(int i=0;i<M;i++)\{ //par}
    \emphh{for(int j=0;j<N;j++)\{ //par}
      \emp{Atr[k,j,i] = A[k,i,j];}
\} \} \}
\end{colorcode}
\column{0.60\textwidth}\vspace{-2ex}
\begin{colorcode}
__global__ void sgmMatTranspose( float* A,  
      float* trA, int rowsA, int colsA ) \{
  __shared__ float tile[T][T+1];
  \emp{int gidz=blockIdx.z*blockDim.z*threadIdx.z;}
  \emp{A+=gidz*rowsA*colsA; Atr+=gidz*rowsA*colsA;}
  \emp{// follows code for matrix transp in x \& y}
  int tidx = threadIdx.x, tidy = threadIdx.y;
  int j=blockIdx.x*T+tidx,i=blockIdx.y*T+tidy;
  if( j < colsA && i < rowsA )
    tile[tidy][tidx] = A[i*colsA+j];
  __syncthreads();
  \blue{i=blockIdx.y*T+tidx; j=blockIdx.x*T+tidy;}
  \blue{if( j < colsA && i < rowsA )}
    \blue{trA[j*rowsA+i] = tile[tidx][tidy];}
\}
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}



\section{Project Code: A Bit More Complex Due to a Seq Loop}

\begin{frame}[fragile]
	\tableofcontents[currentsection]
\end{frame}


\begin{frame}[fragile,t]
  \frametitle{Sequential Loop in Between Parallel Loops} % of CPU, Multicores, GPGPU


\begin{itemize}
    \item \emp{Loop of index {\tt t} is sequential} because
            it reads the array {\tt myResult[k,0:M-1,0:N-1]}
            produced by previous iteration {\tt t-1}! 
    \item Distribute the outermost loop across the two loop nests,
            then interchange to get \emp{loop of index t} in the
            outermost position: 
%\item Loop distribution followed by loop interchange to get
%        the sequential loop outside.
%\item Then loop distribution again to create your kernels. 
\end{itemize}

\begin{block}{Code after Array Expansion {\tt~~~~~~} Get Seq Loop Outside}
\begin{columns}
\column{0.47\textwidth}
\begin{colorcode}
float myResult[Outer, M, N];
\emphh{for(int k=0;j<Outer;k++)\{ //par}
  \emphh{for(int i=0; i<M; i++) //par}
    \emphh{for(int j=0; j<N; j++) //par}
      \emp{myResult[k,i,j] = ...;}

  \emp{for(int t=0;t<T;t++)\{ //seq}
    \emphh{for(int i=0; i<M; i++) //par}
      \emphh{for(int j=0; j<N; j++) //par}
        ... = ... \emp{myResult[k,i,j]} ... ;
    \emphh{for(int i=0; i<M; i++) //par}
      \emphh{for(int j=0; j<N; j++) //par}
        \emp{myResult[k,i,j]} = ...;
  \emp{\}} 
\emphh{\}}
\end{colorcode}
\column{0.47\textwidth}
\begin{colorcode}
float myResult[Outer, M, N];
\emphh{for(int k=0;j<Outer;k++)\{ //par}
  \emphh{for(int i=0; i<M; i++) //par}
    \emphh{for(int j=0; j<N; j++) //par}
      \emp{myResult[k,i,j] = ...;}
\}
\emp{for(int t=0;t<T;t++)\{ //seq}
  \emphh{for(int k=0;j<Outer;k++)\{ //par}
    \emphh{for(int i=0; i<M; i++) //par}
      \emphh{for(int j=0; j<N; j++) //par}
        ... = ... \emp{myResult[k,i,j]} ... ;
    \emphh{for(int i=0; i<M; i++) //par}
      \emphh{for(int j=0; j<N; j++) //par}
        \emp{myResult[k,i,j]} = ...;
\emp{\}} \emphh{\}} 
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}

\begin{frame}[fragile,t]
  \frametitle{Sequential Loop in Between Parallel Loops} % of CPU, Multicores, GPGPU

\begin{itemize}
    \item Finally, distribute again loop {\tt k} against the
            two inner loop nests to create CUDA kernels!
          Then optimize coalescing, etc.!
\end{itemize}

\begin{block}{After Distrib Kernels 2 \& 3 are called inside a Sequential Loop}
\begin{columns}
\column{0.45\textwidth}
\begin{colorcode}
float myResult[Outer, M, N];
\emphh{for(int k=0;j<Outer;k++)\{ //par}
  \emphh{for(int i=0; i<M; i++) //par}
    \emphh{for(int j=0; j<N; j++) //par}
      \emp{myResult[k,i,j] = ...;}
\}
\emp{for(int t=0;t<T;t++)\{ //seq}
  \emphh{for(int k=0;j<Outer;k++)\{ //par}
    \emphh{for(int i=0; i<M; i++) //par}
      \emphh{for(int j=0; j<N; j++) //par}
        ... = ... \emp{myResult[k,i,j]} ... ;
    \emphh{for(int i=0; i<M; i++) //par}
      \emphh{for(int j=0; j<N; j++) //par}
        \emp{myResult[k,i,j]} = ...;
\emp{\}} \emphh{\}} 
\end{colorcode}
\column{0.49\textwidth}
\begin{colorcode}
float myResult[Outer, M, N];
\emphh{for(int k=0;j<Outer;k++)\{  //Kernel1}
  \emphh{for(int i=0; i<M; i++)   //Kernel1}
    \emphh{for(int j=0; j<N; j++) //Kernel1}
      \emp{myResult[k,i,j] = ...;}
\}
\emp{for(int t=0;t<T;t++)\{ //seq}
  \emphh{for(int k=0;j<Outer;k++)\{ //Kernel2}
    \emphh{for(int i=0; i<M; i++)  //Kernel2}
      \emphh{for(int j=0; j<N; j++)//Kernel2}
        ... = ... \emp{myResult[k,i,j]} ... ;
  \emphh{\}}
  \emphh{for(int k=0;j<Outer;k++)\{ //Kernel3}
    \emphh{for(int i=0; i<M; i++)  //Kernel3}
      \emphh{for(int j=0; j<N; j++)//Kernel3}
        \emp{myResult[k,i,j]} = ...;
\emp{\}} \emphh{\}} 
\end{colorcode}
\end{columns}
\end{block} 

\end{frame}

\end{document}

