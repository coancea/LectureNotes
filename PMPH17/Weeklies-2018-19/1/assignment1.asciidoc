= First Weekly Assignment for the PMPH Course

This is the text of the first weekly assignment for the DIKU course
"Programming Massively Parallel Hardware", 2018-2019.

Hand in your solution in the form of a short report in text or PDF
format, along with whatever source-code files are
mandated by specific subtasks. All code files should be zipped 
together into an archive named `wa1-code.zip`.

== Task 1: Prove the Optimized Map-Reduce Lemma

This task refers to the List Homomorphism Promotion Lemmas, which were presented in the first lecture (Monday the 4th of September) and can be found in the lecture notes at page 17-19 inside Section 2.4.2 entitled "Other List-Homomorphism Lemmas".

Your task is to use the three List Homomorphism Promotion Lemmas to prove the following invariant (Theorem 4 in lecture notes):

----
(reduce (+) 0) . (map f)

            ==

(reduce (+) 0) . (map ( (reduce (+) 0) . (map f) ) ) . distr_p
----
 

where `distr_p`  distributes the original list into a list of `p` sublists, each sublist having about the same number of elements, and where `.` denotes the operator for function composition.   Include the solution in the written (text) report. 

*Big Hint:* Please observe that  `(reduce (++) []) . distr_p = id` where `id` is the identity function:
----
reduce (++) [] (distr_p x) == x`, for any list `x`
----
So you should probably start by composing the identity at the end of the first term and then apply the rewrite rules that match:
----
(reduce (+) 0) . (map f) .  (reduce (++) []) . distr_p == ...
----
You should be done deducing the equivalent (second) term of the identity after three steps, each applying a different lemma. Should be a short solution!


== Task 2: Longest Satisfying Segment (LSS) Problem

Your task is to fill in the dots in the implementation of the LSS problem. Please see Sections 2.5.2 and 2.5.3 in lecture notes, pages 21-22. Your task is to

* implement LSS problem in Futhark by filing in the missing lines in file `lssp.fut`, which is attached on the assignment page.

** add one or more small datasets---reference input and output written directly in the `lssp.fut` file---for each predicate (zeros, sorted, same) and make sure that your program validates on all three predicates by running
`futhark-test --compiler=futhark-opencl lssp.fut`

** compile the program with acceleration `futhark-opencl lssp.fut` and without acceleration (sequential C) `futhark-c lssp.fut` and report the speedup of the accelerated version by running both on a large dataset, for example with the command: 
`futhark-dataset --i32-bounds=1:3 -g i32 --i32-bounds=-100:100 -g [10000000]i32 | ./lssp -t /dev/stderr -r 10`

** submit your code (the fully implemented `lssp.fut` file)

* include in your written (text) report the speedups obtained and the validation tests you added, together with your solution, i.e., the missing five lines in Section 2.5.3 of lecture notes.


== Task 3: Flat Sparse-Matrix Vector Multiplication in Futhark

This task refers to writing a flat-parallel version of sparse-matrix vector multiplication in Futhark.
Take a look at Section 3.2.4 ``Sparse-Matrix Vector Multiplication'' in lecture notes, page 40-41 (and potentially also at rewrite rule 5 in Section 4.1.6 ``Flattening a Reduce Directly Nested in a Map'' 
in lecture notes).  The sequential version of the code is attached as `spMVmult-seq.fut`, can be compiled
with `futhark-c spMVmult-seq.fut` and run with 

----
$ futhark-test --compiler=futhark-c spMVmult-seq.fut

$ futhark-c spMVmult-seq.fut

$ futhark-dataset --i32-bounds=0:9999 -g [1000000]i32 --f32-bounds=-7.0:7.0 -g [1000000]f32 --i32-bounds=100:100 -g [10000]i32 --f32-bounds=-10.0:10.0 -g [10000]f32 | ./spMVmult-seq -t /dev/stderr -r 10 > /dev/null
----

However, your task is to fill in a flat-parallel implementation in file `spMVmult-flat.fut`, function `spMatVctMult`, which currently contains a dummy implementation. Add two more standard reference input/output dataset to the source file and measure speedup with respect to the sequential version. The parallel version, once implemented can be tested with

----
$ futhark-test --compiler=futhark-opencl spMVmult-flat.fut
----

and bigger datasets can be generated and run with something like:

----
$ futhark-opencl spMVmult-flat.fut

$ futhark-dataset --i32-bounds=0:9999 -g [1000000]i32 --f32-bounds=-7.0:7.0 -g [1000000]f32 --i32-bounds=100:100 -g [10000]i32 --f32-bounds=-10.0:10.0 -g [10000]f32 | ./spMVmult-seq -t /dev/stderr -r 10 > /dev/null
----

Please submit:

* the `spMVmult-flat.fut` file once implemented and tested.

* In the written (text) report add:
** the flat-parallel implementation of the `spMatVctMult` function and a short explanation of what each line is doing.
** a short explanation about the speedup of your accelerated version in comparison with `spMVmult-seq.fut`


== Task 4: CUDA exercise (see lab 1 slides: Lab1-CudaIntro)

Write a CUDA program with two functions that both map the function (x/(x-2.3))^3 to the array [1,...,753411], i.e., of size 753411. The first function should implement a serial map performed on the CPU; the second function should implement a parallel map in CUDA performed on the GPU.   Check that the result on CPU is equal to the result on GPU (modulo an epsilon error), and print a VALID or INVALID message. Also print the runtime (excluding the time for CPU-to-GPU transfer and GPU memory allocation) taken by the sequential and CUDA implementations.

Measure the time taken for both functions and write a max 5 line explanation why one is better than the other:

* Play with the size of the array and find out where is the sweet-point when the GPU starts being faster than the CPU

* Also increase the size of the array to determine what is roughly the maximal speedup.

Please submit:

* your program named `wa1-task4.cu` together with a `Makefile` for it.
* write in your written report:
** whether it validates (and what epsilon have you used for validating the CPU to GPU results)
** the 5-line explanation of the speedups
** the code of your CUDA kernel together with how it was called, including the code for the computation of the grid and block sizes.
